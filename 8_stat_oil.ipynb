{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_stat_oil.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNISUMpGGNp1h+MQx8iYGb3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/classification/blob/main/8_stat_oil.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqVyg9IRTWd5"
      },
      "source": [
        "\"\"\"\n",
        "Last amended: 20th Nov, 2020\n",
        "Myfolder: https://www.kaggle.com/c/statoil-iceberg-classifier-challenge\n",
        "Ref Kaggle:\n",
        "   https://www.kaggle.com/c/statoil-iceberg-classifier-challenge\n",
        "\n",
        "Good examples:\n",
        "    https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide\n",
        "\n",
        "Ref: PCA vs TruncatedSVD\n",
        "   https://stats.stackexchange.com/questions/239481/difference-between-scikit-learn-implementations-of-pca-and-truncatedsvd\n",
        "Evaluation: For each id in the test set, you\n",
        "            must predict the probability that\n",
        "            the image contains an iceberg or\n",
        "            a ship (a number between 0 and 1).\n",
        "\n",
        "Objectuves:\n",
        "            1. Learn working of lightgbm\n",
        "            2. lightgbm\n",
        "            3. Singular Value Decomposition\n",
        "            4. Cross-validation in python\n",
        "            5. Learning curves\n",
        "            6. Feature importance\n",
        "            7. Bayesian optimization\n",
        "            8. Bayesian optimization using skoptimize\n",
        "               (For Bayesian optimization using hyperopt-sklearn\n",
        "\t\tsee folder 16.hyperopt. This method does not use\n",
        "\t \tGaussian Processes to search for next hyperparameter\n",
        "\t\tpoint.)\n",
        "\n",
        "\n",
        "Further study examples:\n",
        "    https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dPdf8OpTdUI"
      },
      "source": [
        "\n",
        "########################## A.Libraries ##################\n",
        "## 1.0 Call needed libraries\n",
        "\n",
        "%reset -f              # Clear all variables\n",
        "import gc\n",
        "gc.collect()           # Garbage collection\n",
        "\n",
        "# 1.1 Load pandas, numpy and matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib qt5\n",
        "\n",
        "# Image manipulation\n",
        "from matplotlib.pyplot import imshow, imsave\n",
        "#from skimage.io import imshow, imsave\n",
        "\n",
        "# 1.2 Image normalizing and compression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# 1.3 Libraries for splitting, cross-validation & measuring performance\n",
        "from sklearn.model_selection import train_test_split\n",
        "# 1.3.1 Return stratified folds. The folds are made by\n",
        "#        preserving the percentage of samples for each class.\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import auc, roc_curve\n",
        "\n",
        "# 1.4 ML - we will classify using lightgbm\n",
        "#          with stratified cross validation\n",
        "import lightgbm as lgb\n",
        "\n",
        "# 1.5 OS related\n",
        "import os, time\n",
        "\n",
        "# 1.6 Bayes Optimization -- One method\n",
        "#  Install as:\n",
        "#       pip install bayesian-optimization\n",
        "#from bayes_opt import BayesianOptimization\n",
        "\n",
        "# 1.7 Bayes optimization--IInd method\n",
        "# SKOPT is a parameter-optimisation framewor\n",
        "#  Install skopt as:\n",
        "#       conda install -c conda-forge scikit-optimize\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f-3rBoLUJqG"
      },
      "source": [
        "# 3.1\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8Nr257IUEie"
      },
      "source": [
        "\n",
        "# 1.8 Set option to dislay many rows\n",
        "pd.set_option('display.max_columns', 100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKXKcKZMWm_X"
      },
      "source": [
        "## MOUNT gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItrX-K9PURNH"
      },
      "source": [
        "## 2.0 Read files\n",
        "#      filenames\n",
        "#inputFolder = \"C:\\\\Users\\\\ashok\\\\OneDrive\\\\Documents\\\\statoil\\\\\"\n",
        "#inputFolder = \"C:\\\\Users\\\\ashok\\\\Desktop\\\\chmod\\\\lubuntu_machinelearning_I\\\\12.statoil\\\\\"\n",
        "inputFolder = \"/home/ashok/Documents/12.statoil/\"\n",
        "#inputFolder = \"D:\\\\data\\\\OneDrive\\\\Documents\\\\statoil\\\\\"\n",
        "inputFolder = \"C:\\\\Users\\\\Administrator\\\\OneDrive\\\\Desktop\\\\cbi\\\\12.statoil\\\\\"\n",
        "\n",
        "os.chdir(inputFolder)\n",
        "trfile = 'train.json.zip'\n",
        "\n",
        "# 2.1\n",
        "#testfile = 'test.json'\n",
        "\n",
        "# 2.2\n",
        "train = pd.read_json(inputFolder+trfile)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOVtkrK_UW-n"
      },
      "source": [
        "################### B. Understand data ##################\n",
        "################  No data processing here #############\n",
        "\n",
        "\n",
        "### Define some needed functions\n",
        "\n",
        "# 3.0 Examine any dataset\n",
        "#     ExamineData.__doc__  => Gives help\n",
        "def ExamineData(x):\n",
        "    \"\"\"Prints various data charteristics, given x\n",
        "    \"\"\"\n",
        "    print(\"Data shape:\", x.shape)\n",
        "    print(\"\\nColumns:\", x.columns)\n",
        "    print(\"\\nData types\\n\", x.dtypes)\n",
        "    print(\"\\nDescribe data\\n\", x.describe())\n",
        "    print(\"\\nData\\n\", x.head(2))\n",
        "    print (\"\\nSize of data:\", np.sum(x.memory_usage()))    # Get size of dataframes\n",
        "    print(\"\\nAre there any NULLS\\n\", np.sum(x.isnull()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ5-PMFHUoQd"
      },
      "source": [
        "# 3.1 Let us understand train data\n",
        "ExamineData(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ5klQS8UsLR"
      },
      "source": [
        "# 3.2 Look at the first data-point\n",
        "# 3.2.1\n",
        "train.columns\n",
        "train['band_1'].head(3)         # Each point in the Series is a list\n",
        "train['band_1'][0]              # First data point\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_f0C_22Ur0Y"
      },
      "source": [
        "# 3.2.2\n",
        "len(train['band_1'][0] )        # 5625 = 75 X 75 pixels\n",
        "train['band_1'][0][:4]          # Look at first four pixel-values\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaPueiA-U382"
      },
      "source": [
        "### Data within a band can be transfromed\n",
        "### into an array and manipulated\n",
        "\n",
        "# 3.2.3\n",
        "b= train['band_1'].values          # Get complete column (of data-points)\n",
        "b                                #  is transformed to array of points\n",
        "                                #    And each data-point in this array\n",
        "                                #     is a list.\n",
        "\n",
        "type(b)    # numpy.ndarray\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldCiuCa1U43Z"
      },
      "source": [
        "# 3.2.4\n",
        "b.shape    # So how many lists: 1604\n",
        "\n",
        "# 3.2.5\n",
        "j = 5                           # Get jth list\n",
        "b[j]                            # Look at the jth list or data-point in array\n",
        "b[j][:4]                        # Within the jth list (data-point), look at\n",
        "                                #  Ist four pixel values\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-II1LTtSU8lf"
      },
      "source": [
        "# 3.3 How many values this data-point has\n",
        "#len(train['band_1'][0])     # 5625 : 75 X 75\n",
        "\n",
        "\n",
        "# 3.4 Plot the image contained at the first data-point\n",
        "img0 = b[0]\n",
        "type(img0)                     # list\n",
        "img0 = np.array(img0)          # array\n",
        "g = img0.reshape(75,75)        # Same as: np.array(b[0]).reshape(75,75)\n",
        "imshow(g)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkSxS7e4VAu2"
      },
      "source": [
        "################### C. Define useful functions ##################\n",
        "################  No data processing here #############\n",
        "\n",
        "\n",
        "# 4.0 Expand each data-point of band_1/band_2 into\n",
        "#     an array of 5625 points\n",
        "#     For 1604 rows, we get a matrix of size\n",
        "#     1604 X 5625\n",
        "\n",
        "def transform_band_to_matrix(df, band_name):\n",
        "    \"\"\"df: A dataframe\n",
        "       band_name: which of the two bands\n",
        "    \"\"\"\n",
        "    # 4.1 An array of lists. Each point in the array\n",
        "    #     is a list\n",
        "    b =   df[band_name].values\n",
        "    # 4.2 Create a zero-filled-matrix of size\n",
        "    #     1604 X 5625 images\n",
        "    #     Each row of matrix will hold one image\n",
        "    #     from the band\n",
        "    x = np.zeros((1604, 5625))  # 1604 X 5625\n",
        "    # 4.3 For every row-point\n",
        "    for j in range(1604):\n",
        "        # 4.4 For every jth list in 'b' ie b[j]\n",
        "        #     Populate jth row of x, x[j,],\n",
        "        #     with all elements of list b[j]\n",
        "        jth_list = b[j]    # jth point of array is a list\n",
        "        #    Try:  y=[1,2,3] ; np.array(y)\n",
        "        x[j, :] = np.array(jth_list)\n",
        "    return(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PknNohBxVF07"
      },
      "source": [
        "\"\"\"\n",
        "Why have a zero filled array:\n",
        "=============================\n",
        "    An alternative way of creating\n",
        "    a matrix 1604 X 5625 is:\n",
        "        a. Get b[0] & b[1]\n",
        "        b. Transform both of them to array\n",
        "            as, np.array(b[0])\n",
        "        c. Vertically stack them using np.vstack\n",
        "        d. Then get b[3]. Stack it under\n",
        "           earleir stack\n",
        "\n",
        "    But such stacking is not gauranteed to give\n",
        "    a matrix with adjacent memory locations\n",
        "\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuKBGwC7VKad"
      },
      "source": [
        "# 5.0 Given a matrix of 1605 X 5625 from above function,\n",
        "#     plot first six images after reshaping each row\n",
        "#     (or flattened) array to 75 X 75. That is plot\n",
        "#     x[0], x[1], ...x[5] after reshaping each\n",
        "#     to 75 X 75\n",
        "def plot_first_six_Images(x):\n",
        "    \"\"\"x: matrix of size 1604 X 5625\n",
        "       Given a image matrix with each row\n",
        "       as an image, plot images in Ist\n",
        "       six rows.\n",
        "    \"\"\"\n",
        "    # 5.1 Create figure-window and axes\n",
        "    _, ax = plt.subplots(nrows = 2, ncols= 3)\n",
        "    # 5.2\n",
        "    ax[0,0].imshow(x[0, :].reshape(75,75))\n",
        "    ax[0,1].imshow(x[1, :].reshape(75,75))\n",
        "    ax[0,2].imshow(x[2, :].reshape(75,75))\n",
        "    ax[1,0].imshow(x[3, :].reshape(75,75))\n",
        "    ax[1,1].imshow(x[4, :].reshape(75,75))\n",
        "    ax[1,2].imshow(x[5, :].reshape(75,75))\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lZub8ymVN8u"
      },
      "source": [
        "# 6.0 Save an one image file to disk just to look at its size\n",
        "def SaveOneImg(x, filename):\n",
        "    \"\"\"x is matrix of row-wise images\n",
        "       Each image in one row\n",
        "       There are as many images as there are rows\n",
        "       Saves just one image in the very first row\n",
        "       Returns saved file size\n",
        "    \"\"\"\n",
        "    # 6.1 Min and Max pixel intensity values\n",
        "    #     in the first row\n",
        "    lower = np.min(x[0,:])\n",
        "    upper = np.max(x[0,:])\n",
        "    # 6.2 Range of values\n",
        "    range = upper - lower\n",
        "    # 6.3 Normalize now\n",
        "    trans = (x[0,:] - lower)/range           # Normalize image to values [0,1]\n",
        "    imsave(filename, trans.reshape(75,75))   # Reshape image and save it to disk\n",
        "    return os.path.getsize(filename)         # Return img size on disk\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHdlTbjkVRs1"
      },
      "source": [
        "# 7.0 Singular value decomposition of\n",
        "#     of matrix 1604 X 5625\n",
        "#     So we have 5625 columns\n",
        "#     See SVD vs PCA below\n",
        "def SingularValueDecomp(x, n_comp):\n",
        "    \"\"\"x is a matrix of images\n",
        "         or a dataset with 5625 columns\n",
        "         It is returned from function:\n",
        "         transform_band_to_matrix().\n",
        "       n_comp: Number of SVD components\n",
        "       This function returns three objects:\n",
        "         a. transformed matrix,\n",
        "         b. transformation object, and\n",
        "         c. explained variance\n",
        "    \"\"\"\n",
        "    # 7.1 Create an object to perform SVD\n",
        "    svd = TruncatedSVD(n_components = n_comp)\n",
        "    # 7.2 Fit and transform\n",
        "    g = svd.fit_transform(x)\n",
        "    # 7.3 % of variance explained by each component\n",
        "    ev1 = svd.explained_variance_ratio_\n",
        "    # Return a tuple of three values\n",
        "    return (g, svd, ev1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxlVWOL7VVbP"
      },
      "source": [
        "\n",
        "################### D. Data processing ################\n",
        "\n",
        "# 9.0 There are some 'na' values in 'inc_angle' column\n",
        "#     We will fill these up with mean value. Note\n",
        "#     that the word 'na' is string not np.nan\n",
        "\n",
        "# 9.1 Following is a pandas Series with boolean values\n",
        "train['inc_angle'] == 'na'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAxV4VTcVZIV"
      },
      "source": [
        "\n",
        "# 9.2 How many such rows are there?\n",
        "np.sum(train['inc_angle'] == 'na')   # 133 only\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7HdyAlRVcSA"
      },
      "source": [
        "# 9.3 OK. Get now the mean of non-na rows\n",
        "m_value = np.mean(train.loc[train['inc_angle'] != 'na', 'inc_angle' ])\n",
        "m_value       # 39.26870747790618\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBZtou6CVfa3"
      },
      "source": [
        "\n",
        "# 9.4 Replace 'na' with mean-value\n",
        "train.loc[train['inc_angle'] == 'na', 'inc_angle' ] = m_value\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOArQWR7Vi9V"
      },
      "source": [
        "# 9.5 Finally check\n",
        "train['inc_angle']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEh8D3P3VmpN"
      },
      "source": [
        "# 9.6 Create a categorical variable from inc_angle\n",
        "train['c_angle'] = 1\n",
        "train.loc[train['inc_angle'] > m_value, 'c_angle'] = 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnqVOVcVVtF9"
      },
      "source": [
        "################### E. Pre Analyses  ##################\n",
        "\n",
        "\n",
        "### Our steps in Modeling\n",
        "##  Brief steps: ################\n",
        "#   1. Reshape single column, band_1, into an array 5625 columns\n",
        "#   2. Reshape single column, band_2, into 5625 columns\n",
        "#   3. Truncate 5625 cols of band_1 to 25 cols using SVD\n",
        "#   4. Truncate 5625 cols of band_2 to 25 cols using SVD\n",
        "#   5. Concatenate two expanded column. Result: 50 cols\n",
        "#   6. Cut inc_angle into two, to create a categorical\n",
        "#      variable having two categories\n",
        "#   7. To (5) above, concatenate two morecolumns:\n",
        "#      One: of 'inc_angle' and Two: categorical column\n",
        "#      created in step (6)\n",
        "#   8. Do modeling\n",
        "#   9. While modeling tune parameters of model using Bayes\n",
        "#      optimization technique\n",
        "################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWm2WmuSVwW3"
      },
      "source": [
        "# 10.0 Examine train/test data\n",
        "ExamineData(train)    # 1604 X 5   155MB\n",
        "# 10.1\n",
        "#ExamineData(test)     # 8424 X 5   817MB\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkAaebiIV0Bu"
      },
      "source": [
        "## 11. Popualte a variable with flattened images\n",
        "#      from band_1.\n",
        "#      Band 1 and Band 2 are signals characterized\n",
        "#      by radar backscatter produced from different\n",
        "#      polarizations at a particular incidence angle.\n",
        "matrix_band1 = transform_band_to_matrix(train, \"band_1\")\n",
        "matrix_band1.shape           # 1604 X 5625\n",
        "# 12 Also plot six of the satellite images\n",
        "plot_first_six_Images(matrix_band1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDfpNRVyV3N9"
      },
      "source": [
        "## 13. Popualte another variable with flattened images from band_2\n",
        "matrix_band2 = transform_band_to_matrix(train, \"band_2\")\n",
        "matrix_band2.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBn0fQ0LV8Fx"
      },
      "source": [
        "\n",
        "# 13.1 Also plot six images from it\n",
        "plot_first_six_Images(matrix_band2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPBoEy5mV84C"
      },
      "source": [
        "\n",
        "# 13.1 Also plot six images from it\n",
        "plot_first_six_Images(matrix_band2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvb0e4ZJWErA"
      },
      "source": [
        "\n",
        "## 14. Save one image from band1 into a file\n",
        "#      Returned value is file size\n",
        "filesize = SaveOneImg(matrix_band1, \"band1.jpg\")\n",
        "filesize                # 1695 bytes\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdxw5XK8V_p2"
      },
      "source": [
        "\n",
        "# 15. Perform Singular Value Decomposition\n",
        "#    Get how much variance is explained per-component\n",
        "#    in band1. Start with total of 500 components\n",
        "_,_,ev1 = SingularValueDecomp(matrix_band1, 500)\n",
        "_,_,ev2 = SingularValueDecomp(matrix_band2, 500)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDMp2KkdWJC2"
      },
      "source": [
        "\n",
        "# 16. Plot the cumulative sum of explained variances\n",
        "#     component-wise for both the bands\n",
        "plt.figure()\n",
        "frac_band1 = np.cumsum(ev1)\n",
        "frac_band2 = np.cumsum(ev2)\n",
        "plt.plot(frac_band1[:200])         # 25 components appear OK\n",
        "plt.plot(frac_band2[:200])         # 25 components appear OK\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rokl4X5EWNT2"
      },
      "source": [
        "\n",
        "# 17. Next, let us , therefore, truncate to 25 components\n",
        "#     Singular value decomposition of two bands of train data\n",
        "#      So new flattened images are in comp1 and in comp2\n",
        "comp1,svd1, _ = SingularValueDecomp(matrix_band1,25)\n",
        "comp2,svd2, _ = SingularValueDecomp(matrix_band2, 25)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw_2QlfjWP7A"
      },
      "source": [
        "\n",
        "# 18. We can perform inverse_transform with just 25 svd components\n",
        "#     and relook at images (75 X 75) to visually see how much of\n",
        "#     information relating to original image does exist\n",
        "im1 = svd1.inverse_transform(comp1)\n",
        "im2 = svd2.inverse_transform(comp2)\n",
        "im1.shape         # Shape remains as 1604 X 5625, as earlier\n",
        "im2.shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCIveCJMWVD-"
      },
      "source": [
        "\n",
        "# 19. Now, relook at transformed and original images\n",
        "#     Transformed first\n",
        "plot_first_six_Images(im1)\n",
        "# 19.1 Compare above with original. Not bad..\n",
        "plot_first_six_Images(band1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6Of3FTVWXy_"
      },
      "source": [
        "\n",
        "## 20. Does SVD compress image?\n",
        "##     Save transformed image\n",
        "#      And compare its size with that of original image\n",
        "#      saved earlier\n",
        "##     Obviously SVD compresses image\n",
        "SaveOneImg(im1, \"im1.jpg\")      # Size: 713 bytes as against 1695 bytes\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4xvnU1CWjfP"
      },
      "source": [
        "\n",
        "################### F. Modeling  ##################\n",
        "\n",
        "### 21. Finally we horizontally stack flattened images\n",
        "#       Note here that comp1 and comp2 may be at\n",
        "#       difft memory locations.\n",
        "X = np.hstack((comp1, comp2))\n",
        "X.shape\n",
        "X[0]     # Check Ist row\n",
        "X[:2]    # Check Ist two rows\n",
        "\n",
        "\n",
        "# 21.1 Also stack inc_angle\n",
        "X.shape         # 1604 X 50\n",
        "X = np.hstack((X, train.inc_angle.values.reshape(X.shape[0],1)))\n",
        "X.shape          # (1604,51)\n",
        "X = np.hstack((X, train.c_angle.values.reshape(X.shape[0],1)))\n",
        "X.shape      # 1604 X 52                Predictors\n",
        "\n",
        "X[0]         # Check again Ist row\n",
        "X[:2]       # Check again Ist two rows\n",
        "\n",
        "y = train['is_iceberg'].values         # Target\n",
        "\n",
        "\n",
        "# 21.2 Partition into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "                                     X, y,\n",
        "                                     test_size=0.30,\n",
        "                                     random_state=42\n",
        "                                     )\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "################# Bayesian optimization-I with skopt ##########################\n",
        "# Refer:\n",
        "# https://www.kaggle.com/nanomathias/bayesian-optimization-of-xgboost-lb-0-9769\n",
        "# Refer for LGBMRegressor:\n",
        "#   https://lightgbm.readthedocs.io/en/latest/Python-API.html#lightgbm.LGBMRegressor\n",
        "# And  https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
        "\n",
        "# Our modeling object for Classification\n",
        "# Can be any modeler: RandoForest, xgboost, neuralnetwork\n",
        "#                     svm etc\n",
        "\n",
        "# 22. Store train and test data in lightgbm,\n",
        "#     Dataset object\n",
        "\n",
        "\n",
        "model = lgb.LGBMRegressor(                # Regressor will also perform classification\n",
        "                          objective='binary',\n",
        "                          metric='auc',   # This output must match with what\n",
        "                                          #  we specify as input to Bayesian model\n",
        "                          n_jobs=2,\n",
        "                          verbose=0\n",
        "                          )\n",
        "\n",
        "# 22.1 Parameter search space for selected modeler\n",
        "#      For suggested parameter grid for lightgbm, pl see: \n",
        "#         https://github.com/Microsoft/LightGBM/issues/695\n",
        "#\n",
        "params = {\n",
        "        'boosting' :   Categorical(['gbdt','rf','dart']),\n",
        "        'n_estimators': Integer(50, 100),  # Number of boosted trees or iterations to fit (default: 100).\n",
        "\n",
        "        'num_leaves':   Integer(5,45),     # Maximum tree leaves for base boosters\n",
        "                                           # Create a node only if no of leaves exceed this limit\n",
        "                                           #  and also following condition of 'min_child_samples' is met\n",
        "\n",
        "        'min_child_samples': Integer(1, 50),  # Create a node only if min data-points at this node\n",
        "                                              #  exceed this limit\n",
        "\n",
        "        'feature_fraction': Real(0.1, 0.9),\t   # Randomly select part of features on each iteration\n",
        "                                               #  for every boosted tree\n",
        "\n",
        "        'bagging_fraction': Real(0.8, 1),\t   # Randomly select part of data without resampling\n",
        "                                               #   for each boosted tree\n",
        "\n",
        "        'bagging_freq'   : Integer(1,10),      # k means perform bagging at every k iteration\n",
        "\n",
        "        'max_depth' : Integer(1, 50),          # Maximum tree depth for base learners, -1 means no limit.\n",
        "\n",
        "        'learning_rate': Real(0.01, 1.0, 'log-uniform'), # Prob of interval 1 to 10 is same as 10 to 100\n",
        "                                                         # Equal prob of selection from 0.01 to 0.1, 0.1\n",
        "                                                         # to 1\n",
        "                                                         # In a loguniform distributon, log-transformed\n",
        "                                                         # random variable is uniformly distributed\n",
        "\n",
        "        'reg_lambda': Real(1e-9, 1000, 'log-uniform'),  # L2 regularization term on weights.\n",
        "        'reg_alpha': Real(1e-9, 1.0, 'log-uniform'),    #  L1 regularization\n",
        "\n",
        "\n",
        "        'scale_pos_weight': Real(1, 10),       # default: 1\n",
        "                                               # used only in binary application\n",
        "                                               # How much more importance should be given to binary\n",
        "                                               # weight of labels with positive class\n",
        "\n",
        "\n",
        "        #-----***** Not understood **** -----\n",
        "\n",
        "        'max_bin': Integer(100, 1000),        # max number of bins that feature\n",
        "                                              #  values will be bucketed in\n",
        "                                              # small number of bins may reduce\n",
        "                                              # training accuracy but may increase\n",
        "                                              # general power (deal with over-fitting)\n",
        "\n",
        "        'min_child_weight': Real(1, 10),      # Deals with overfitting\n",
        "\n",
        "        'subsample_for_bin': Integer(100000, 500000)  #  Number of samples for constructing bins(default: 200000)\n",
        "                                               # setting this to larger value will give better training\n",
        "                                               #  result, but will increase data loading time\n",
        "\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "# 22.2 Cross validation strategy for the modeler\n",
        "#      Perform startified k-fold cross-validation\n",
        "#      There is also RepeatedStratifiedKFold() class\n",
        "#      that will repeat startified k-fold N-number\n",
        "#      of times\n",
        "#      Instantiate cross-vlidation object\n",
        "\"\"\"\n",
        "Examples of Cross-validation strategies:\n",
        "    i)   Leave one out  : Very time consuming\n",
        "    ii)  Leave P out    : For example, leave 2 out\n",
        "    iii) kfold          : k-equal random folds\n",
        "    iv)  StratifiedKFold : kfolds + stratification\n",
        "    v)   ShuffleSplit  => Generate n-numbers of userdefined pairs\n",
        "                          of (train,test). For examples, in each\n",
        "                          (train,test) pair, let number of rows\n",
        "                          of 'test' data be 30% of train data\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "cvStrategy = StratifiedKFold(\n",
        "                             n_splits=3,\n",
        "                             shuffle=True,\n",
        "                             random_state=42\n",
        "                            )\n",
        "\n",
        "\n",
        "\n",
        "# 22.3 Bayesian object instantiation\n",
        "#     For API, refer: https://scikit-optimize.github.io/#skopt.BayesSearchCV\n",
        "#     For example: https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html\n",
        "bayes_cv_tuner = BayesSearchCV(\n",
        "                              estimator = model,    # rf, lgb, xgb, nn , pipeline etc--Black box\n",
        "                              search_spaces = params,  # Specify params as required by the estimator\n",
        "                              scoring = 'roc_auc',  # Input to Bayes function\n",
        "                                                    # modeler should return this\n",
        "                                                    # peformence metric\n",
        "                              cv = cvStrategy,      # Optional. Determines the cross-validation splitting strategy.\n",
        "                                                    #           Can be cross-validation generator or an iterable,\n",
        "                                                    #           Possible inputs for cv are: - None, to use the default 3-fold cv,\n",
        "                                                    #           - integer, to specify the number of folds in a (Stratified)KFold,\n",
        "                                                    #           - An object to be used as a cross-validation generator.\n",
        "                              n_jobs = 2,           # Start two parallel threads for processing\n",
        "                              n_iter = 50,        # How many times to look for parameter sets\n",
        "                              verbose = 1,\n",
        "                              refit = True,       #  Refit the best estimator with the entire dataset\n",
        "                              random_state = 42\n",
        "                               )\n",
        "\n",
        "# 22.4 Start learning using Bayes tuner\n",
        "start = time.time()\n",
        "result = bayes_cv_tuner.fit(\n",
        "                           X_train,       # Note that we use normal train data\n",
        "                           y_train       #  rather than lgb train-data matrix\n",
        "                           #callback=status_print\n",
        "                           )\n",
        "\n",
        "end = time.time()\n",
        "(end - start)/60\n",
        "\n",
        "\n",
        "# 22.5 So what are the results?\n",
        "#      Use the following estimator in future\n",
        "bayes_cv_tuner.best_estimator_\n",
        "# 22.6 What parameters the best estimator was using?\n",
        "best_params = pd.Series(bayes_cv_tuner.best_params_)\n",
        "best_params\n",
        "\n",
        "# 22.7 Best auc score for the above estimator\n",
        "np.round(bayes_cv_tuner.best_score_, 4)\n",
        "\n",
        "# 22.8 Summary of all models developed by Bayes process\n",
        "allModels_summary = pd.DataFrame(bayes_cv_tuner.cv_results_)\n",
        "allModels_summary.shape  # 50 X 26\n",
        "allModels_summary.head()\n",
        "\n",
        "\n",
        "### 23. Let us now use the best estimator\n",
        "bst_bayes = bayes_cv_tuner.best_estimator_\n",
        "bst_bayes\n",
        "# 23.1 Train the best estimator\n",
        "bst_bayes.fit(X_train, y_train)\n",
        "# 23.2 Make predictions\n",
        "pred = bst_bayes.predict(X_test)\n",
        "pred\n",
        "\n",
        "# 23.3 So what is auc score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, pred, pos_label=1)\n",
        "auc(fpr, tpr)    # 94%\n",
        "\n",
        "\n",
        "############## Learning Curve with lightgbm #############\n",
        "## What is a Learning Curve--See note at the end.\n",
        "\n",
        "# Ref: https://lightgbm.readthedocs.io/en/latest/Python-API.html#training-api\n",
        "# 24. Store train and test data in lightgbm,\n",
        "#     Dataset object\n",
        "\n",
        "\n",
        "d_train = lgb.Dataset(X_train, label=y_train) # transformed train data\n",
        "d_test = lgb.Dataset(X_test, label = y_test)  # test data\n",
        "\n",
        "# 24.1 Watch error in these datasets as\n",
        "#      modeling proceeds\n",
        "watchlist = [d_train, d_test]\n",
        "\n",
        "## 25. Build Lightgbm model\n",
        "# Set parameters first\n",
        "# Ref: http://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
        "#      https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
        "\n",
        "params = { 'learning_rate': 0.25,\n",
        "           'verbosity': -1,             # Be verbose when processing\n",
        "           'categorical_feature' : [51],  # which cols are categorical\n",
        "                                          # (specify index 0,1,2..)\n",
        "            'nthread': 4,                 # USe CPU cores\n",
        "            'max_depth': 7,            # limit the max depth for tree model\n",
        "            'objective' : \"binary\",\n",
        "            'metric' : ['auc', 'binary_logloss']\n",
        "           }\n",
        "\n",
        "\n",
        "## 26\n",
        "start = time.time()\n",
        "# 26.1\n",
        "evals_result = {} # to record eval results for plotting\n",
        "model = lgb.train(params,\n",
        "                  train_set=d_train,\n",
        "                  num_boost_round=1500,     # 1000 residuals are mapped to functions\n",
        "                                            #   successively\n",
        "                  valid_sets=watchlist,\n",
        "                  early_stopping_rounds=100, # The goal of early stopping is to\n",
        "                                            #  decide if any of the latest X rounds\n",
        "                                            #  has improved performance versus a baseline,\n",
        "                                            #  according to some metric.\n",
        "                                            # The model will train until the validation\n",
        "                                            #  score stops improving. Validation error\n",
        "                                            #  needs to improve at least every\n",
        "                                            #  early_stopping_rounds to continue training.\n",
        "                 evals_result=evals_result, # Record evaluation results for plotting\n",
        "                 verbose_eval=10\n",
        "                 )\n",
        "\n",
        "# 26.2\n",
        "end = time.time()\n",
        "end - start\n",
        "\n",
        "# 27. Plot learning curve\n",
        "print('Plot metrics during training...')\n",
        "ax = lgb.plot_metric(evals_result, metric='binary_logloss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 28. Plot precision/Recall curve\n",
        "#  Ref: https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py\n",
        "from sklearn.metrics import precision_recall_curve as pr\n",
        "y_pred = model.predict(X_test)\n",
        "precision,recall,_ = pr(y_test,y_pred)   # Reruns a tuple of three arrays\n",
        "                                         # precision, recall, thresholds\n",
        "plt.plot(precision,recall)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "######## FEATURE IMPORTANCE ###########\n",
        "\n",
        "# 29\n",
        "# Column wise imporatnce. Default Criteria: \"split\".\n",
        "# \"split\":  Result contains numbers of times feature is used in a model.\n",
        "# “gain”:   Result contains total information-gains of splits\n",
        "#           which use the feature\n",
        "print('Plot feature importances...')\n",
        "ax = lgb.plot_importance(bst_bayes, max_num_features=10)\n",
        "ax.tick_params(labelsize=20)\n",
        "plt.show()\n",
        "\n",
        "# 29.1 Does not work. Needs 'graphviz'\n",
        "ax= lgb.plot_tree(bst_bayes,\n",
        "                  tree_index=9,\n",
        "                  figsize=(40, 20),\n",
        "                  show_info=['split_gain'])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#################### Bayesian-optimization-II Normal method ###################\n",
        "# Ref: https://github.com/fmfn/BayesianOptimization\n",
        "\n",
        "# 25. Create lightgbm dataset, a binary file\n",
        "#     LightGBM binary file\n",
        "#     Also saving Dataset into a LightGBM binary file will make loading faster:\n",
        "d_train = lgb.Dataset(X_train, label=y_train) # transformed train data\n",
        "d_test = lgb.Dataset(X_test, label = y_test)  # test data\n",
        "\n",
        "\n",
        "# 25.1\n",
        "#  Step 1: Create a function that when passed some parameters\n",
        "#          evaluates results using cross-validation\n",
        "def lgb_eval(num_leaves,feature_fraction, bagging_fraction,max_depth):\n",
        "    # Specify complete list of parameters: static and dynamic\n",
        "    # 25.2 Static: Parameters that need not be modified\n",
        "    params = {'application':'binary',\n",
        "              'num_boost_round':4000,\n",
        "              'learning_rate':0.05,\n",
        "              'early_stopping_round':100,\n",
        "              'metric':'auc',\n",
        "              'shuffle':True\n",
        "              }\n",
        "\n",
        "    # 25.3 Dynamic: Parameters that would be passed using arguments to function\n",
        "    params[\"num_leaves\"] = int(round(num_leaves))\n",
        "    params['feature_fraction'] = feature_fraction\n",
        "    params['bagging_fraction'] = bagging_fraction\n",
        "    params['max_depth'] = int(round(max_depth))\n",
        "\n",
        "    # 25.4 Now evaluate with above parameters\n",
        "    cv_result = lgb.cv(params,\n",
        "                       d_train,\n",
        "                       nfold=4,\n",
        "                       seed=0,\n",
        "                       stratified=True,\n",
        "                       #verbose_eval =200,\n",
        "                       metrics=['auc'])\n",
        "\n",
        "    # 25.5 Finally return maximum value of result\n",
        "    return max(cv_result['auc-mean'])\n",
        "\n",
        "\n",
        "# 26. Step 2: Define BayesianOptimization function has\n",
        "#             two arguments\n",
        "lgbBO = BayesianOptimization(\n",
        "                             lgb_eval,               # Which function will evaluate\n",
        "\n",
        "                             # Parameters to tune and to be\n",
        "                             #   passed to above function\n",
        "                             # Specify parameter range for each\n",
        "                             {'num_leaves': (24, 45),\n",
        "                              'feature_fraction': (0.1, 0.9),\n",
        "                              'bagging_fraction': (0.8, 1),\n",
        "                              'max_depth': (5, 8.99)\n",
        "                              }\n",
        "                             )\n",
        "\n",
        "# 26.1. Gaussian process parameters\n",
        "gp_params = {\"alpha\": 1e-5}      # Initialization parameter for gaussian\n",
        "                                 # process\n",
        "\n",
        "# 27. Step 3: Start optimization\n",
        "start = time.time()\n",
        "lgbBO.maximize(init_points=2,    # Number of randomly chosen points to\n",
        "                                 # sample the target function before\n",
        "                                 #  fitting the gaussian Process (gp)\n",
        "                                 #  or gaussian graph\n",
        "               n_iter=25         # Total number of times the\n",
        "                                 #   process is to repeated\n",
        "               )\n",
        "end = time.time()\n",
        "f\"Minutes: {(end -start)/60}\"\n",
        "\n",
        "\n",
        "\n",
        "# 28. Get results\n",
        "lgbBO.max\n",
        "lgbBO.max['params']['max_depth']\n",
        "lgbBO.max['params']['bagging_fraction']\n",
        "lgbBO.max['params']['feature_fraction']\n",
        "lgbBO.max['params']['num_leaves']\n",
        "\n",
        "# 29. Newly discovered parameter values\n",
        "params_new = lgbBO.max['params']\n",
        "params_new['num_leaves'] = int(params_new['num_leaves']) + 1\n",
        "params_new['max_depth'] = int(params_new['max_depth']) + 1\n",
        "\n",
        "\n",
        "# 29.1 Objective alias 'application'\n",
        "params_new['objective'] = ['binary']    # Default regression\n",
        "\n",
        "# 29.2 Metric parameters\n",
        "params_new['metric'] = ['auc', 'binary_logloss']   # Multiple loss parameters\n",
        "\n",
        "\n",
        "# 29.3 Watch error in these datasets as\n",
        "#      modeling proceeds\n",
        "watchlist = [d_train, d_test]\n",
        "\n",
        "\n",
        "# 30\n",
        "start = time.time()\n",
        "model = lgb.train(params_new,\n",
        "                  train_set=d_train,\n",
        "                  num_boost_round=1000,     # 1000 residuals are mapped to functions\n",
        "                                            #   successively\n",
        "                  valid_sets=watchlist,\n",
        "                  early_stopping_rounds=20, # The goal of early stopping is to\n",
        "                                            #  decide if any of the latest X rounds\n",
        "                                            #  has improved performance versus a baseline,\n",
        "                                            #  according to some metric.\n",
        "                                            # The model will train until the validation\n",
        "                                            #  score stops improving. Validation error\n",
        "                                            #  needs to improve at least every\n",
        "                                            #  early_stopping_rounds to continue training.\n",
        "                  evals_result=evals_result, # Record evaluation results for plotting\n",
        "                  verbose_eval=10)\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "end - start\n",
        "\n",
        "# 30.1\n",
        "model.best_score\n",
        "\n",
        "# 30.2 Save model to a text file for later use\n",
        "model.save_model('model.txt',\n",
        "                 num_iteration=model.best_iteration\n",
        "                 )\n",
        "\n",
        "# 30.3 Delete model\n",
        "del model\n",
        "\n",
        "# 30.4 Load back saved model\n",
        "bst = lgb.Booster(model_file='model.txt')  #init model\n",
        "\n",
        "# 30.5 If early stopping is enabled during training,\n",
        "#      get predictions from the best iteration with\n",
        "#      bst.best_iteration\n",
        "\n",
        "lgb_pred = bst.predict(X_test,\n",
        "                       num_iteration=bst.best_iteration)  > 0.5\n",
        "lgb_pred\n",
        "y_test\n",
        "\n",
        "# 30.6 Now accuracy\n",
        "np.sum(lgb_pred == y_test)/y_test.size\n",
        "# 30.7 So what is auc score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, lgb_pred, pos_label=1)\n",
        "auc(fpr, tpr)\n",
        "\n",
        "\n",
        "\n",
        "#####################################################\n",
        "################ SVD vs PCA ###################\n",
        "\"\"\"\n",
        "Ref:\n",
        "   https://stats.stackexchange.com/a/87536/78454\n",
        "\n",
        "SVD is slower than PCA but is often considered to be the preferred\n",
        "method because of its higher numerical accuracy.\n",
        "As you state in the question, principal component analysis (PCA) can\n",
        "be carried out either by SVD of the centered data matrix X.\n",
        "\n",
        "Matlab's help records this:\n",
        "Principal component algorithm that pca uses to perform the\n",
        "principal component analysis [...]:\n",
        "[PCA uses two methods, svd and eigenvalue BUT unlike in SVD\n",
        "PCA is done (on column-wise) centered data]\n",
        "\n",
        "\n",
        "\n",
        " i)    'svd' -- Default. Singular value decomposition (SVD) of X.\n",
        "                         Slower but more accurate\n",
        " ii)   'eig' -- Eigenvalue decomposition (EIG) of the covariance matrix.\n",
        "                The EIG algorithm is faster than SVD when the number of\n",
        "                observations, n, exceeds the number of variables, p, but\n",
        "                is less accurate because the condition number of the\n",
        "                covariance is the square of the condition number of X.\n",
        "                Faster but less accurate\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Learning Curve--General definition\n",
        "Ref: https://stackoverflow.com/a/13715276\n",
        "\n",
        "    One line:  How a model performs as some hyperparameter is varied\n",
        "\n",
        "    A learning curve conventionally depicts improvement in performance\n",
        "    on the vertical axis when there are changes in another parameter\n",
        "    (on the horizontal axis), such as training set size (in machine learning)\n",
        "    or iteration/time (in both machine and biological learning). One salient\n",
        "    point is that many parameters of the model are changing at different points\n",
        "    on the plot.\n",
        "\n",
        "    \"\"\"\n",
        "###############################\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}